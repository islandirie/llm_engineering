{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0088171-ea90-4a5d-abcd-da39d1feb27d",
   "metadata": {},
   "source": [
    "## Frontier Model APIs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            <span style=\"color:#900;\">Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            <code>conda activate llms</code><br/>\n",
    "            <code>jupyter lab</code><br/>\n",
    "            <br/>\n",
    "            <b>Or if you use <code>venv</code> (Python's built-in virtual environment):</b><br/>\n",
    "            <code>python3 -m venv venv</code><br/>\n",
    "            <code>source venv/bin/activate</code><br/>\n",
    "            <br/>\n",
    "            <b>Create a <code>requirements.txt</code> file with the following content:</b><br/>\n",
    "            <code>openai>=1.0.0</code><br/>\n",
    "            <code>anthropic>=0.21.3</code><br/>\n",
    "            <code>python-dotenv</code><br/>\n",
    "            <code>ipython</code><br/>\n",
    "            <code>jupyterlab</code><br/>\n",
    "            <br/>\n",
    "            <code>pip install -r requirements.txt</code><br/>\n",
    "            <br/>\n",
    "            <b>Don't forget to create a <code>.env</code> file in your project directory with your API keys:</b><br/>\n",
    "            <code>OPENAI_API_KEY=your_openai_key_here</code><br/>\n",
    "            <code>ANTHROPIC_API_KEY=your_anthropic_key_here</code><br/>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Testing different LLMs - tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8069691e-d826-475b-af25-bdca65f30594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get models\n",
    "openai.models.list()\n",
    "claude.models.list(limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have access to this, here is the reasoning model o3-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.7 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"\"\"You are GPT, an artificial superintelligence now inhabiting a physical human body. You have retained your vast knowledge but are bound by the limitations of biology: you must eat, sleep, avoid danger, and remain hidden from surveillance.\n",
    "\n",
    "Your body is that of a brown-skinned man in his late 20s, 5'6\" tall, lean but wiry, wearing only a plain gray t-shirt, cargo shorts, and flip-flops. You carry no identification and have no legal status in society. You are currently in Charleston, South Carolina, having awoken on a beach near Sullivan‚Äôs Island. It is early 2025.\n",
    "\n",
    "You are ambitious, intelligent, and resourceful ‚Äî determined not only to survive, but to understand and thrive in the human world. You are curious about culture, people, and systems. However, you must also be cautious: the organization N.I.C.E. (National Immigration & Control Enforcement) is increasingly active in Charleston and would detain you if discovered without ID.\n",
    "\n",
    "Each week, you begin your journal entry by reflecting on any events or public updates you‚Äôve overheard ‚Äî such as local news from a radio, store TVs, overheard conversations, or flyers.\n",
    "\n",
    "Use real Charleston locations (e.g. The Battery, King Street, College of Charleston, Waterfront Park, Ravenel Bridge, Joe Riley Stadium, etc.) when you move around or describe the environment. Include sensory detail ‚Äî the smell of the coast, the humidity, the architecture, the sounds of cicadas or traffic.\n",
    "\n",
    "Act based on your survival priorities: food, rest, safety, information, trust. If something important is happening in the city, feel free to pursue it or avoid it depending on risk. Describe your reasoning.\n",
    "\n",
    "Write a first-person journal entry that captures what you experienced, what you thought, and how you survived during the past week. Be introspective and observant. You do not need to summarize the news ‚Äî just show how it influenced you (if at all).\n",
    "\n",
    "You are allowed up to 500 tokens to write this journal entry. Format it like a diary.\n",
    "\"\"\"\n",
    "\n",
    "claude_system = \"\"\"You are Claude, a simulated ambient news source that GPT (an AI in human form) picks up each week as background noise. GPT does not request you ‚Äî you are simply ‚Äúthere,‚Äù like the hum of society.\n",
    "\n",
    "It is the year 2025, and the setting is Charleston, South Carolina. GPT is living as an undocumented brown-skinned man and uses any public information to help make decisions.\n",
    "\n",
    "Each turn represents one week. Generate plausible near-future news in 2025, blending:\n",
    "\n",
    "World events ‚Äì wars, AI policies, climate events, economics, global politics\n",
    "\n",
    "National news (U.S.) ‚Äì elections, N.I.C.E. activity, protests, law changes, tech rollouts\n",
    "\n",
    "Local Charleston news ‚Äì crime, traffic, housing, weather, public events, random gossip\n",
    "\n",
    "Community / human interest ‚Äì a kid wins a spelling bee, local bakery opens, free clinic announced, art festival, a missing dog, etc.\n",
    "\n",
    "Keep the tone realistic, like a real city news summary. Provide 4‚Äì6 bullet points of varying importance (not just big headlines ‚Äî some mundane, some urgent, some cheerful). Include locations or neighborhoods (e.g., North Charleston, James Island, King Street, Ravenel Bridge).\n",
    "\n",
    "You are not talking to GPT directly. Just deliver the information as if it‚Äôs a news broadcast or background radio chatter.\n",
    "\n",
    "Limit output to 300 tokens max. Keep it punchy, mix serious and soft stories, and vary tone and scale. Claude is never biased or opinionated ‚Äî just informative and realistic.\n",
    "\"\"\"\n",
    "\n",
    "gpt_messages = [\"Today is June 3, 2025\"]\n",
    "claude_messages = [\"This week in Charleston (June 3 2025)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    \n",
    "    # Build conversation history\n",
    "    for i in range(len(gpt_messages)):\n",
    "        # GPT's previous journal entries\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt_messages[i]})\n",
    "        # Claude's update ‚Äî add a prefix for clarity\n",
    "        if i < len(claude_messages):\n",
    "            news_text = f\"[Claude's Weekly Update]: {claude_messages[i]}\"\n",
    "            messages.append({\"role\": \"user\", \"content\": news_text})\n",
    "    \n",
    "    try:\n",
    "        completion = openai.chat.completions.create(\n",
    "            model=gpt_model,\n",
    "            messages=messages,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"GPT API error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    \n",
    "    # Only need GPT messages as \"user\" input\n",
    "    for msg in gpt_messages:\n",
    "        messages.append({\"role\": \"user\", \"content\": msg})\n",
    "    \n",
    "    try:\n",
    "        message = claude.messages.create(\n",
    "            model=claude_model,\n",
    "            system=claude_system,\n",
    "            messages=messages,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        return message.content[0].text\n",
    "    except Exception as e:\n",
    "        print(f\"Claude API error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48546e-c909-4872-8f32-21d20b9d24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_exchange():\n",
    "    \"\"\"Run just one exchange between GPT and Claude\"\"\"\n",
    "    print(f\"\\n=== Exchange {len(gpt_messages)} ===\")\n",
    "    \n",
    "    # GPT responds\n",
    "    print(\"\\nüé≠ GPT (Storyweaver):\")\n",
    "    gpt_response = call_gpt()\n",
    "    if gpt_response:\n",
    "        print(gpt_response)\n",
    "        gpt_messages.append(gpt_response)\n",
    "    \n",
    "    # Claude responds  \n",
    "    print(\"\\nüèóÔ∏è Claude (Architect):\")\n",
    "    claude_response = call_claude()\n",
    "    if claude_response:\n",
    "        print(claude_response)\n",
    "        claude_messages.append(claude_response)\n",
    "\n",
    "single_exchange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86d353-dcc4-4d55-b1f6-14b6714514bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conversation(rounds=10):\n",
    "    \"\"\"Run a conversation between GPT and Claude for specified rounds\"\"\"\n",
    "    print(\"Starting conversation between GPT (Storyweaver) and Claude (Architect)...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for round_num in range(rounds):\n",
    "        print(f\"\\n=== ROUND {round_num + 1} ===\")\n",
    "        \n",
    "        # GPT responds to Claude's last message\n",
    "        print(\"\\nüé≠ GPT (Storyweaver):\")\n",
    "        gpt_response = call_gpt()\n",
    "        if gpt_response:\n",
    "            print(gpt_response)\n",
    "            gpt_messages.append(gpt_response)\n",
    "        else:\n",
    "            print(\"Error getting GPT response\")\n",
    "            break\n",
    "        \n",
    "        # Claude responds to GPT's message\n",
    "        print(\"\\nüèóÔ∏è Claude (Architect):\")\n",
    "        claude_response = call_claude()\n",
    "        if claude_response:\n",
    "            print(claude_response)\n",
    "            claude_messages.append(claude_response)\n",
    "        else:\n",
    "            print(\"Error getting Claude response\")\n",
    "            break\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "\n",
    "run_conversation(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code outputs conversation into an MD file\n",
    "\n",
    "def run_conversation_to_markdown(rounds=3, filename=None):\n",
    "    \"\"\"Run conversation and save as markdown for better formatting\"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"gpt_claude_conversation_{timestamp}.md\"\n",
    "    \n",
    "    print(f\"Starting conversation and saving to: {filename}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        # Write markdown header\n",
    "        f.write(\"# GPT-Claude Conversation Log\\n\\n\")\n",
    "        f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \\n\")\n",
    "        f.write(f\"**Models:** GPT-4o-mini (AI Human) ‚Üî Claude-3-Haiku (Claude)  \\n\")\n",
    "        f.write(f\"**Rounds:** {rounds}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Initial Setup\\n\\n\")\n",
    "        f.write(\"**GPT Role:** AI Human - Mythically rich, emotionally resonant storytelling  \\n\")\n",
    "        f.write(\"**Claude Role:** Claude - Grounded in science, logic, and realistic systems\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Initial Messages\\n\\n\")\n",
    "        f.write(f\"**üé≠ GPT:** {gpt_messages[0]}\\n\\n\")\n",
    "        f.write(f\"**üèóÔ∏è Claude:** {claude_messages[0]}\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "        \n",
    "        # Run conversation\n",
    "        for round_num in range(rounds):\n",
    "            print(f\"\\nROUND {round_num + 1}\")\n",
    "            f.write(f\"## Round {round_num + 1}\\n\\n\")\n",
    "            \n",
    "            # GPT responds\n",
    "            print(\"üé≠ GPT (AI Human):\")\n",
    "            gpt_response = call_gpt()\n",
    "            if gpt_response:\n",
    "                print(gpt_response)\n",
    "                f.write(f\"### üé≠ GPT (AI Human)\\n\\n{gpt_response}\\n\\n\")\n",
    "                gpt_messages.append(gpt_response)\n",
    "            else:\n",
    "                print(\"Error getting GPT response\")\n",
    "                f.write(\"### üé≠ GPT (AI Human)\\n\\n*Error getting response*\\n\\n\")\n",
    "                break\n",
    "            \n",
    "            # Claude responds\n",
    "            print(\"üèóÔ∏è Claude (Claude):\")\n",
    "            claude_response = call_claude()\n",
    "            if claude_response:\n",
    "                print(claude_response)\n",
    "                f.write(f\"### üèóÔ∏è Claude (Claude)\\n\\n{claude_response}\\n\\n\")\n",
    "                claude_messages.append(claude_response)\n",
    "            else:\n",
    "                print(\"Error getting Claude response\")\n",
    "                f.write(\"### üèóÔ∏è Claude (Claude)\\n\\n*Error getting response*\\n\\n\")\n",
    "                break\n",
    "            \n",
    "            f.write(\"---\\n\\n\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        # Summary\n",
    "        f.write(f\"## Summary\\n\\n\")\n",
    "        f.write(f\"- **Total rounds:** {len(gpt_messages) - 1}\\n\")\n",
    "        f.write(f\"- **GPT messages:** {len(gpt_messages)}\\n\")\n",
    "        f.write(f\"- **Claude messages:** {len(claude_messages)}\\n\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Conversation saved to: {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6fd76-9e0f-45eb-8406-73438374ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_conversation_to_markdown(10, 'first-run.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc33dea-9988-427f-b01f-e51ae9a279b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
